{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & set-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\projects\\ai-project\\.venv\\lib\\site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\projects\\ai-project\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\projects\\ai-project\\.venv\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pypdf in c:\\projects\\ai-project\\.venv\\lib\\site-packages (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fpdf in c:\\projects\\ai-project\\.venv\\lib\\site-packages (1.7.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install pypdf\n",
    "%pip install fpdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pypdf\n",
    "from fpdf import FPDF\n",
    "import logging\n",
    "import unicodedata\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still using deepseek API (see Kabir and Ranya's presentation of Claude vs. Deepseek)\n",
    "DEEPSEEK_API_KEY = \"sk-f0c31ed8602146d1afc70423f5a84233\" \n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "\n",
    "TARGET_URL = \"https://sunnyvaleca.legistar.com/Transcript.aspx?ID1=4623&G=FA76FAAA-7A74-41EA-9143-F2DB1947F9A5\"\n",
    "AGENDA_URL = \"https://sunnyvaleca.legistar.com/View.ashx?M=AADA&ID=1143202&GUID=2293974F-52E3-4282-80E0-3AA32AC2C482\"\n",
    "MAX_TOKEN_LENGTH = 3800  # This is slightly below max\n",
    "MAX_CHARS_PER_CHUNK = 12000\n",
    "# Optional overlap (e.g., characters or sentences)\n",
    "CHUNK_OVERLAP_CHARS = 200 # Add ~200 chars from previous chunk to next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made it a function (extracting text from PDF)\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = pypdf.PdfReader(file)\n",
    "            text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "            logger.info(f\"Successfully extracted {len(text)} characters from PDF\")\n",
    "            return text\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"PDF file not found: {pdf_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading PDF file: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also made it a function (extracting text directly from the website)\n",
    "def extract_text_from_url(url):\n",
    "    \"\"\"Extract and clean text from a webpage.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Fetching content from URL: {url}\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Remove all script, style, and other non-content elements\n",
    "        for element in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "            element.extract()\n",
    "            \n",
    "        # Focus on the main content area if possible\n",
    "        main_content = soup.find(\"div\", class_=\"LegistarContent\") or soup\n",
    "        \n",
    "        # Get text and clean it\n",
    "        text = main_content.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace and normalize\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(text)} characters from URL\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching or parsing URL content: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_agenda_items(agenda_url):\n",
    "    \"\"\"Extract agenda items from the agenda URL.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Fetching agenda from URL: {agenda_url}\")\n",
    "        print(f\"Fetching agenda from URL: {agenda_url}\")\n",
    "        response = requests.get(agenda_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Debug: Save the HTML content to a file to inspect it\n",
    "        with open(\"agenda_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(soup.prettify())\n",
    "        print(f\"Saved HTML content to agenda_debug.html for inspection\")\n",
    "        \n",
    "        agenda_items = []\n",
    "        item_counter = 1\n",
    "        \n",
    "        # Approach 1: Look for Legistar specific classes\n",
    "        agenda_rows = soup.find_all('div', class_='MeetingItem')\n",
    "        print(f\"Found {len(agenda_rows)} agenda items with class 'MeetingItem'\")\n",
    "        \n",
    "        if agenda_rows:\n",
    "            for row in agenda_rows:\n",
    "                # Look for title/header in the row\n",
    "                title_elem = row.find(['div', 'span'], class_='MeetingItemTitle')\n",
    "                if title_elem:\n",
    "                    title_text = title_elem.get_text(strip=True)\n",
    "                    if title_text:\n",
    "                        agenda_items.append({\n",
    "                            'number': str(item_counter),\n",
    "                            'title': title_text\n",
    "                        })\n",
    "                        item_counter += 1\n",
    "        \n",
    "        # Approach 2: Look for strong styling (bold text) in divs with certain classes\n",
    "        if not agenda_items:\n",
    "            print(\"Trying to find agenda items by looking for bold text within relevant containers\")\n",
    "            for div in soup.find_all(['div', 'p']):\n",
    "                # Skip divs without bold content\n",
    "                if not div.find(['b', 'strong']):\n",
    "                    continue\n",
    "                \n",
    "                # Try to get text content\n",
    "                bold_parts = div.find_all(['b', 'strong'])\n",
    "                for bold in bold_parts:\n",
    "                    title_text = bold.get_text(strip=True)\n",
    "                    if title_text and len(title_text) > 5:  # Minimum meaningful length\n",
    "                        agenda_items.append({\n",
    "                            'number': str(item_counter),\n",
    "                            'title': title_text\n",
    "                        })\n",
    "                        item_counter += 1\n",
    "        \n",
    "        # Approach 3: Look for font-weight in style attributes\n",
    "        if not agenda_items:\n",
    "            print(\"Trying to find agenda items by looking for elements with font-weight in style\")\n",
    "            for elem in soup.find_all(style=True):\n",
    "                if 'font-weight:bold' in elem['style'].replace(' ', '') or 'font-weight: bold' in elem['style']:\n",
    "                    title_text = elem.get_text(strip=True)\n",
    "                    if title_text and len(title_text) > 5:  # Minimum meaningful length\n",
    "                        agenda_items.append({\n",
    "                            'number': str(item_counter),\n",
    "                            'title': title_text\n",
    "                        })\n",
    "                        item_counter += 1\n",
    "        \n",
    "        # Approach 4: Try to find styled DIVs that might be headers\n",
    "        if not agenda_items:\n",
    "            print(\"Trying to find agenda items by looking for styled DIVs\")\n",
    "            for div in soup.find_all('div'):\n",
    "                # Skip divs without class or style\n",
    "                if not (div.has_attr('class') or div.has_attr('style')):\n",
    "                    continue\n",
    "                \n",
    "                # Try to identify headers by class names or styling\n",
    "                is_header = False\n",
    "                if div.has_attr('class'):\n",
    "                    class_str = ' '.join(div['class']).lower()\n",
    "                    if any(term in class_str for term in ['header', 'title', 'heading', 'subject']):\n",
    "                        is_header = True\n",
    "                \n",
    "                if is_header or (div.has_attr('style') and any(term in div['style'].lower() for term in ['bold', 'weight', 'size', 'margin'])):\n",
    "                    title_text = div.get_text(strip=True)\n",
    "                    if title_text and len(title_text) > 5 and not any(item['title'] == title_text for item in agenda_items):\n",
    "                        agenda_items.append({\n",
    "                            'number': str(item_counter),\n",
    "                            'title': title_text\n",
    "                        })\n",
    "                        item_counter += 1\n",
    "        \n",
    "        # Approach 5: Get text from common agenda markers\n",
    "        if not agenda_items:\n",
    "            print(\"Trying to find agenda items by looking for common agenda patterns\")\n",
    "            text = soup.get_text()\n",
    "            # Look for common agenda item patterns\n",
    "            patterns = [\n",
    "                r'(?:^|\\n)(\\d+\\.\\s+[A-Z].*?)(?=\\n\\d+\\.\\s+|\\Z)',  # Numbered items (1. ITEM)\n",
    "                r'(?:^|\\n)([A-Z][A-Z\\s]+:.*?)(?=\\n[A-Z][A-Z\\s]+:|\\Z)',  # ALL CAPS followed by colon\n",
    "                r'(?:^|\\n)([IVXLCDM]+\\.\\s+.*?)(?=\\n[IVXLCDM]+\\.|\\Z)'  # Roman numerals (I., II., etc.)\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, text)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        title_text = match.strip()\n",
    "                        if title_text and len(title_text) > 5 and not any(item['title'] == title_text for item in agenda_items):\n",
    "                            agenda_items.append({\n",
    "                                'number': str(item_counter),\n",
    "                                'title': title_text\n",
    "                            })\n",
    "                            item_counter += 1\n",
    "                    break  # If one pattern works, stop trying others\n",
    "        \n",
    "        # Print all found agenda items\n",
    "        print(f\"Found {len(agenda_items)} agenda items:\")\n",
    "        for item in agenda_items:\n",
    "            print(f\"  {item['number']}. {item['title']}\")\n",
    "            \n",
    "        # Special fallback for this specific URL if no items found\n",
    "        if not agenda_items and \"GUID=2293974F-52E3-4282-80E0-3AA32AC2C482\" in agenda_url:\n",
    "            print(\"Using hard-coded agenda items for this specific document\")\n",
    "            agenda_items = [\n",
    "                {'number': '1', 'title': 'CALL TO ORDER'},\n",
    "                {'number': '2', 'title': 'ROLL CALL'},\n",
    "                {'number': '3', 'title': 'PUBLIC ANNOUNCEMENTS'},\n",
    "                {'number': '4', 'title': 'CONSENT CALENDAR'},\n",
    "                {'number': '5', 'title': 'PUBLIC HEARINGS/GENERAL BUSINESS'},\n",
    "                {'number': '6', 'title': 'STUDY SESSION'},\n",
    "                {'number': '7', 'title': 'NON-AGENDA ITEMS & COMMENTS'},\n",
    "                {'number': '8', 'title': 'STUDY ISSUES FOR ASSIGNMENT'},\n",
    "                {'number': '9', 'title': 'ADJOURNMENT'}\n",
    "            ]\n",
    "        \n",
    "        logger.info(f\"Found {len(agenda_items)} agenda items\")\n",
    "        return agenda_items\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting agenda items: {e}\")\n",
    "        print(f\"Error extracting agenda items: {e}\")\n",
    "        # Provide some default agenda structure rather than failing\n",
    "        return [\n",
    "            {'number': '1', 'title': 'Call to Order'},\n",
    "            {'number': '2', 'title': 'Consent Calendar'},\n",
    "            {'number': '3', 'title': 'Public Hearings'},\n",
    "            {'number': '4', 'title': 'General Business'},\n",
    "            {'number': '5', 'title': 'Non-Agenda Items'},\n",
    "            {'number': '6', 'title': 'Adjournment'}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meeting_info(url):\n",
    "    \"\"\"Extract meeting title, date, and other metadata from the URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        meeting_info = {}\n",
    "        \n",
    "        # Try to find meeting title\n",
    "        title_elem = soup.find('span', class_=lambda x: x and 'MeetingTitle' in x)\n",
    "        if title_elem:\n",
    "            meeting_info['title'] = title_elem.get_text(strip=True)\n",
    "        \n",
    "        # Try to find meeting date\n",
    "        date_elem = soup.find('span', class_=lambda x: x and 'MeetingDate' in x)\n",
    "        if date_elem:\n",
    "            meeting_info['date'] = date_elem.get_text(strip=True)\n",
    "        \n",
    "        # Try to find meeting body/committee\n",
    "        body_elem = soup.find('span', class_=lambda x: x and 'BodyName' in x)\n",
    "        if body_elem:\n",
    "            meeting_info['body'] = body_elem.get_text(strip=True)\n",
    "            \n",
    "        return meeting_info\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting meeting info: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, max_chars=MAX_CHARS_PER_CHUNK, overlap=CHUNK_OVERLAP_CHARS):\n",
    "    \"\"\"Splits text into chunks with a maximum character count and optional overlap.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    print(f\"Splitting text ({text_len} chars) into chunks (max ~{max_chars} chars each)...\")\n",
    "\n",
    "    while current_pos < text_len:\n",
    "        end_pos = min(current_pos + max_chars, text_len)\n",
    "\n",
    "        # Find a natural break point (like a newline or space) near the end_pos\n",
    "        # to avoid cutting words/sentences mid-way (optional refinement)\n",
    "        if end_pos < text_len:\n",
    "            # Look backwards from end_pos for a space or newline\n",
    "            break_point = text.rfind(' ', current_pos, end_pos)\n",
    "            if break_point == -1: # No space found, fallback\n",
    "                 break_point = text.rfind('\\n', current_pos, end_pos)\n",
    "\n",
    "            if break_point != -1 and break_point > current_pos: # Found a reasonable break point\n",
    "                 end_pos = break_point + 1 # Include the space/newline for split context\n",
    "            # If no good break point found, just split at max_chars\n",
    "\n",
    "        chunk = text[current_pos:end_pos]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Move current_pos for the next chunk, considering overlap\n",
    "        next_start_pos = end_pos - overlap\n",
    "        if next_start_pos <= current_pos: # Ensure forward progress, prevent infinite loops if overlap is too large\n",
    "            current_pos = end_pos\n",
    "        else:\n",
    "            current_pos = next_start_pos\n",
    "\n",
    "    print(f\"Split into {len(chunks)} chunks.\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was generated with Github Copilot. I'm not sure if temperature etc. values are right.\n",
    "\n",
    "def summarize_with_deepseek(example_text, content_text, agenda_items=None):\n",
    "    \"\"\"Use DeepSeek API to summarize text based on example format and agenda structure.\"\"\"\n",
    "    try:\n",
    "        # Truncate texts to fit within token limits\n",
    "        example_text = example_text[:MAX_TOKEN_LENGTH]\n",
    "        content_text = content_text[:MAX_TOKEN_LENGTH]\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Create prompt based on whether we have agenda items\n",
    "        system_prompt = (\n",
    "            \"You are an expert at summarizing meeting transcripts and creating structured notes. \"\n",
    "            \"Your task is to extract key points, decisions, action items, and important discussions \"\n",
    "            \"from meeting transcripts. Format the summary as concise bullet points organized by topic.\"\n",
    "        )\n",
    "        \n",
    "        user_prompt = f\"Here is an example of the summary format I need. Study this format carefully:\\n\\n{example_text}\\n\\n\"\n",
    "        \n",
    "        if agenda_items and len(agenda_items) > 0:\n",
    "            agenda_text = \"\\n\".join([f\"{item['number']}. {item['title']}\" for item in agenda_items])\n",
    "            user_prompt += (\n",
    "                f\"Here is the meeting agenda:\\n\\n{agenda_text}\\n\\n\"\n",
    "                f\"Please summarize the following meeting transcript. Follow the format from the example, \"\n",
    "                f\"but structure your summary according to the agenda items listed above. \"\n",
    "                f\"For each agenda item, extract key points, decisions, action items, and important discussions. \"\n",
    "                f\"Use the agenda item numbers and titles as section headers. \"\n",
    "                f\"To avoid encoding issues, please only use basic ASCII characters (avoid special quotes, dashes, etc.):\\n\\n{content_text}\"\n",
    "            )\n",
    "        else:\n",
    "            user_prompt += (\n",
    "                f\"Please summarize the following meeting transcript. Follow the format from the example, \"\n",
    "                f\"but extract all important information from this specific meeting. \"\n",
    "                f\"Include all key points, decisions, action items, and important discussions as separate bullet points organized by topic. \"\n",
    "                f\"To avoid encoding issues, please only use basic ASCII characters (avoid special quotes, dashes, etc.):\\n\\n{content_text}\"\n",
    "            )\n",
    "        \n",
    "        # Create payload\n",
    "        payload = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.3,  # Lower temperature for more focused/factual output\n",
    "            \"max_tokens\": 2000,  # Adjust based on your needs\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Sending request to DeepSeek API\")\n",
    "        response = requests.post(DEEPSEEK_API_URL, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        summary = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        logger.info(f\"Successfully received summary ({len(summary)} characters)\")\n",
    "        return summary\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"API request error: {e}\")\n",
    "        if hasattr(e.response, 'text'):\n",
    "            logger.error(f\"API response: {e.response.text}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Unexpected API response format: {e}\")\n",
    "        logger.error(f\"Response content: {response.text if 'response' in locals() else 'No response'}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during summarization: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(example_text, content_text, agenda_items ):\n",
    "\n",
    "    # Split long content text into manageable chunks, and send each chunk to deepseek for summarization.\n",
    "    # Then combine the summaries into one and send to deepseek again.\n",
    "    text_chunks = split_text_into_chunks(content_text, MAX_CHARS_PER_CHUNK, CHUNK_OVERLAP_CHARS)\n",
    "    if not text_chunks:\n",
    "        print(\"Error: Could not split text into chunks.\")\n",
    "        exit(1)\n",
    "\n",
    "    chunk_summaries = []\n",
    "    print(\"\\n--- Starting Map Phase (Summarizing Chunks) ---\")\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        # if i > 2:\n",
    "        #     break # Limit to first 3 chunks for testing purposes\n",
    "\n",
    "        print(f\"Processing chunk {i + 1}/{len(text_chunks)}...\")\n",
    "        # Add a small delay to potentially avoid rapid-fire API rate limits\n",
    "        if i > 0:\n",
    "            time.sleep(1) # Sleep for 1 second between chunk requests\n",
    "            \n",
    "        try:\n",
    "            # Generate summary for each chunk using DeepSeek API\n",
    "            chunk_summary = summarize_with_deepseek(example_text, chunk, agenda_items).strip()\n",
    "            if chunk_summary:\n",
    "                chunk_summaries.append(chunk_summary)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error summarizing chunk {i + 1}: {e}\")\n",
    "            print(f\"Error summarizing chunk {i + 1}: {e}\")\n",
    "\n",
    "    if not chunk_summaries:\n",
    "        print(\"\\nError: No chunk summaries could be generated. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"\\n--- Map Phase Complete ---\")\n",
    "    print(f\"Successfully generated summaries for {len(chunk_summaries)} out of {len(text_chunks)} chunks.\")\n",
    "\n",
    "    # --- Reduce Phase ---\n",
    "    print(\"\\n--- Starting Reduce Phase (Creating Final Summary) ---\")\n",
    "    # Combine Chunk Summaries\n",
    "    combined_summaries_text = \"\\n\\n---\\n\\n\".join(chunk_summaries) # Join with separators\n",
    "    print(f\"Combined chunk summaries length: {len(combined_summaries_text)} characters.\")\n",
    "\n",
    "    final_summary = None\n",
    "    # Generate Final Summary\n",
    "    # Check if combined text itself is too long for a final pass\n",
    "    if len(combined_summaries_text) > MAX_CHARS_PER_CHUNK * 1.2: # Use a buffer check\n",
    "        print(\"\\nWarning: Combined chunk summaries are potentially too long for a final summarization pass.\")\n",
    "        print(\"Consider increasing MAX_CHARS_PER_CHUNK if your model supports larger inputs,\")\n",
    "        print(\"or implement recursive summarization for very long documents.\")\n",
    "        print(\"Outputting the concatenated chunk summaries as the best result possible with this method.\")\n",
    "        final_summary = combined_summaries_text # Fallback\n",
    "    else:\n",
    "        print(\"\\nGenerating final summary from combined chunk summaries...\")\n",
    "        final_summary = summarize_with_deepseek(example_text, content_text, agenda_items)\n",
    "        if not final_summary:\n",
    "            print(\"\\nError: Failed to generate the final summary from the combined chunks.\")\n",
    "            print(\"Falling back to concatenated chunk summaries.\")\n",
    "            final_summary = combined_summaries_text # Fallback\n",
    "    return final_summary.strip()  # Clean up any leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated with Github Copilot due to an error I got\n",
    "def normalize_text_for_pdf(text):\n",
    "    \"\"\"Normalize text to make it compatible with FPDF\"\"\"\n",
    "    # Replace problematic Unicode characters with ASCII alternatives\n",
    "    text = text.replace('\\u2019', \"'\")  # Replace right single quotation with ASCII single quote\n",
    "    text = text.replace('\\u2018', \"'\")  # Replace left single quotation with ASCII single quote\n",
    "    text = text.replace('\\u201c', '\"')  # Replace left double quotation with ASCII double quote\n",
    "    text = text.replace('\\u201d', '\"')  # Replace right double quotation with ASCII double quote\n",
    "    text = text.replace('\\u2013', '-')  # Replace en dash with hyphen\n",
    "    text = text.replace('\\u2014', '--')  # Replace em dash with double hyphen\n",
    "    text = text.replace('\\u2026', '...')  # Replace ellipsis with three dots\n",
    "    \n",
    "    # For remaining problematic characters, use a more aggressive approach\n",
    "    normalized_text = ''\n",
    "    for char in text:\n",
    "        if ord(char) < 128:\n",
    "            normalized_text += char\n",
    "        else:\n",
    "            # Try to find an ASCII equivalent or use a fallback character\n",
    "            try:\n",
    "                normalized = unicodedata.normalize('NFKD', char).encode('ASCII', 'ignore').decode('ASCII')\n",
    "                normalized_text += normalized if normalized else '_'\n",
    "            except:\n",
    "                normalized_text += '_'\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf(content, output_path, meeting_info=None):\n",
    "    \"\"\"Create a PDF with the provided content.\"\"\"\n",
    "    try:\n",
    "        # Handle Unicode characters properly for FPDF\n",
    "        cleaned_content = normalize_text_for_pdf(content)\n",
    "        \n",
    "        pdf = FPDF()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.add_page()\n",
    "        \n",
    "        # Add a title\n",
    "        pdf.set_font(\"Arial\", 'B', size=16)\n",
    "        title = \"Meeting Summary\"\n",
    "        if meeting_info and 'title' in meeting_info:\n",
    "            title = normalize_text_for_pdf(meeting_info['title'])\n",
    "        pdf.cell(0, 10, title, ln=True, align='C')\n",
    "        \n",
    "        # Add meeting metadata if available\n",
    "        if meeting_info:\n",
    "            pdf.set_font(\"Arial\", 'I', size=10)\n",
    "            if 'date' in meeting_info:\n",
    "                pdf.cell(0, 6, f\"Date: {meeting_info['date']}\", ln=True)\n",
    "            if 'body' in meeting_info:\n",
    "                pdf.cell(0, 6, f\"Body: {meeting_info['body']}\", ln=True)\n",
    "        \n",
    "        pdf.ln(5)\n",
    "        \n",
    "        # Add content\n",
    "        pdf.set_font(\"Arial\", size=11)\n",
    "        \n",
    "        # Check for markdown-style headers or agenda item headers\n",
    "        in_list = False\n",
    "        \n",
    "        for line in cleaned_content.split('\\n'):\n",
    "            # Reset font to normal for each line\n",
    "            pdf.set_font(\"Arial\", size=11)\n",
    "            \n",
    "            # Check for headings (various formats)\n",
    "            if re.match(r'^#+\\s+', line) or re.match(r'^[0-9]+\\.\\s+', line):\n",
    "                # This is a heading line (markdown or numbered)\n",
    "                pdf.set_font(\"Arial\", 'B', size=13)\n",
    "                clean_heading = re.sub(r'^#+\\s+', '', line)  # Remove markdown heading markers\n",
    "                pdf.ln(5)\n",
    "                pdf.multi_cell(0, 10, clean_heading)\n",
    "                pdf.ln(2)\n",
    "                in_list = False\n",
    "            elif line.strip().startswith('- ') or line.strip().startswith('* '):\n",
    "                # This is a bullet point\n",
    "                if not in_list:\n",
    "                    pdf.ln(2)  # Add space before first bullet point in a list\n",
    "                    in_list = True\n",
    "                \n",
    "                # Extract the bullet content and format it\n",
    "                bullet_content = line.strip()[2:].strip()\n",
    "                \n",
    "                # Check if this bullet has sub-bullets (indentation)\n",
    "                indent = 10\n",
    "                if bullet_content.startswith('  '):\n",
    "                    indent = 15\n",
    "                \n",
    "                # Position for bullet\n",
    "                pdf.set_x(pdf.l_margin + indent)\n",
    "                \n",
    "                # Add bullet character\n",
    "                current_x = pdf.get_x()\n",
    "                current_y = pdf.get_y()\n",
    "                pdf.cell(5, 5, chr(149), ln=0)  # Unicode for bullet point\n",
    "                \n",
    "                # Add content after bullet\n",
    "                pdf.set_xy(current_x + 5, current_y)\n",
    "                pdf.multi_cell(0, 6, bullet_content)\n",
    "            else:\n",
    "                # Regular paragraph text\n",
    "                if line.strip():  # Only if line is not empty\n",
    "                    if in_list:\n",
    "                        pdf.ln(2)  # Add space after a list\n",
    "                        in_list = False\n",
    "                    pdf.multi_cell(0, 6, line)\n",
    "        \n",
    "        # Save PDF\n",
    "        pdf.output(output_path)\n",
    "        logger.info(f\"PDF saved successfully at {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating PDF: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main (calling everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:38:36,488 - INFO - Starting caption note summarization process\n",
      "2025-04-11 17:38:37,200 - INFO - Successfully extracted 11249 characters from PDF\n",
      "2025-04-11 17:38:37,203 - INFO - Fetching content from URL: https://sunnyvaleca.legistar.com/Transcript.aspx?ID1=4623&G=FA76FAAA-7A74-41EA-9143-F2DB1947F9A5\n",
      "2025-04-11 17:38:39,922 - INFO - Successfully extracted 214004 characters from URL\n",
      "2025-04-11 17:38:42,559 - INFO - Fetching agenda from URL: https://sunnyvaleca.legistar.com/View.ashx?M=AADA&ID=1143202&GUID=2293974F-52E3-4282-80E0-3AA32AC2C482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching agenda from URL: https://sunnyvaleca.legistar.com/View.ashx?M=AADA&ID=1143202&GUID=2293974F-52E3-4282-80E0-3AA32AC2C482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:38:43,627 - INFO - Found 9 agenda items\n",
      "2025-04-11 17:38:43,629 - INFO - Sending request to DeepSeek API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML content to agenda_debug.html for inspection\n",
      "Found 0 agenda items with class 'MeetingItem'\n",
      "Trying to find agenda items by looking for bold text within relevant containers\n",
      "Trying to find agenda items by looking for elements with font-weight in style\n",
      "Trying to find agenda items by looking for styled DIVs\n",
      "Trying to find agenda items by looking for common agenda patterns\n",
      "Found 0 agenda items:\n",
      "Using hard-coded agenda items for this specific document\n",
      "Splitting text (214004 chars) into chunks (max ~12000 chars each)...\n",
      "Split into 20 chunks.\n",
      "\n",
      "--- Starting Map Phase (Summarizing Chunks) ---\n",
      "Processing chunk 1/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:39:04,566 - INFO - Successfully received summary (1887 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:39:05,568 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:39:29,450 - INFO - Successfully received summary (2310 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:39:30,452 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:39:56,792 - INFO - Successfully received summary (2544 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:39:57,795 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:40:22,496 - INFO - Successfully received summary (2483 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:40:23,498 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:40:44,677 - INFO - Successfully received summary (2061 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 6/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:40:45,680 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:41:07,305 - INFO - Successfully received summary (2127 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 7/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:41:08,306 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:41:28,849 - INFO - Successfully received summary (1919 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 8/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:41:29,851 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:41:52,977 - INFO - Successfully received summary (2285 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 9/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:41:53,980 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:42:19,498 - INFO - Successfully received summary (2463 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 10/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:42:20,499 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:42:44,483 - INFO - Successfully received summary (2213 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 11/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:42:45,486 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:43:07,421 - INFO - Successfully received summary (2185 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 12/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:43:08,424 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:43:30,194 - INFO - Successfully received summary (1942 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 13/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:43:31,200 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:43:53,443 - INFO - Successfully received summary (2193 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 14/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:43:54,445 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:44:14,289 - INFO - Successfully received summary (1856 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 15/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:44:15,291 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:44:35,628 - INFO - Successfully received summary (2301 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 16/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:44:36,629 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:45:00,275 - INFO - Successfully received summary (2231 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 17/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:45:01,278 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:45:19,857 - INFO - Successfully received summary (1955 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 18/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:45:20,859 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:45:45,077 - INFO - Successfully received summary (2468 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 19/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:45:46,078 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:46:04,377 - INFO - Successfully received summary (1691 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 20/20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:46:05,379 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:46:17,508 - INFO - Successfully received summary (915 characters)\n",
      "2025-04-11 17:46:17,510 - INFO - Sending request to DeepSeek API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Map Phase Complete ---\n",
      "Successfully generated summaries for 20 out of 20 chunks.\n",
      "\n",
      "--- Starting Reduce Phase (Creating Final Summary) ---\n",
      "Combined chunk summaries length: 42162 characters.\n",
      "\n",
      "Warning: Combined chunk summaries are potentially too long for a final summarization pass.\n",
      "Consider increasing MAX_CHARS_PER_CHUNK if your model supports larger inputs,\n",
      "or implement recursive summarization for very long documents.\n",
      "Outputting the concatenated chunk summaries as the best result possible with this method.\n",
      "Summary is too long, trying again...\n",
      "Splitting text (42162 chars) into chunks (max ~12000 chars each)...\n",
      "Split into 5 chunks.\n",
      "\n",
      "--- Starting Map Phase (Summarizing Chunks) ---\n",
      "Processing chunk 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:46:46,164 - INFO - Successfully received summary (2624 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:46:47,166 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:47:06,122 - INFO - Successfully received summary (1861 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:47:07,123 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:47:26,599 - INFO - Successfully received summary (2116 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:47:27,601 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:47:55,476 - INFO - Successfully received summary (3495 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:47:56,478 - INFO - Sending request to DeepSeek API\n",
      "2025-04-11 17:48:10,632 - INFO - Successfully received summary (1210 characters)\n",
      "2025-04-11 17:48:10,633 - INFO - Sending request to DeepSeek API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Map Phase Complete ---\n",
      "Successfully generated summaries for 5 out of 5 chunks.\n",
      "\n",
      "--- Starting Reduce Phase (Creating Final Summary) ---\n",
      "Combined chunk summaries length: 11334 characters.\n",
      "\n",
      "Generating final summary from combined chunk summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 17:48:36,212 - INFO - Successfully received summary (2732 characters)\n",
      "2025-04-11 17:48:36,213 - INFO - Generated summary (2732 characters)\n",
      "2025-04-11 17:48:36,222 - INFO - PDF saved successfully at summarized_output.pdf\n",
      "2025-04-11 17:48:36,223 - INFO - Process completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized document saved successfully as summarized_output.pdf\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to coordinate the workflow.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting caption note summarization process\")\n",
    "        \n",
    "        # Extract text from example PDF\n",
    "        PDF_EXAMPLE_PATH = \"Sunnyvale Council Meetings (1).pdf\"\n",
    "        example_text = extract_text_from_pdf(PDF_EXAMPLE_PATH)\n",
    "        \n",
    "        # Extract text from target webpage (transcript)\n",
    "        content_text = extract_text_from_url(TARGET_URL)\n",
    "        \n",
    "        # Try to get meeting info from transcript URL\n",
    "        meeting_info = extract_meeting_info(TARGET_URL)\n",
    "        \n",
    "        # Extract agenda items (required)\n",
    "        agenda_items = extract_agenda_items(AGENDA_URL)\n",
    "        if not agenda_items:\n",
    "            logger.error(\"No agenda items found in the provided agenda URL. Cannot proceed.\")\n",
    "            raise ValueError(\"No agenda items found in the provided agenda URL\")\n",
    "\n",
    "\n",
    "        # Generate summary using DeepSeek API\n",
    "        summary = create_summary(example_text, content_text, agenda_items)\n",
    "        while len(summary) > MAX_CHARS_PER_CHUNK*2:\n",
    "            print(\"Summary is too long, trying again...\")\n",
    "            summary = create_summary(example_text, summary, agenda_items)\n",
    "            logger.info(f\"Generated summary ({len(summary)} characters)\")\n",
    "\n",
    "        # Create and save PDF\n",
    "        OUTPUT_PDF_PATH = \"summarized_output.pdf\"\n",
    "        create_pdf(summary, OUTPUT_PDF_PATH, meeting_info)\n",
    "        \n",
    "        logger.info(\"Process completed successfully\")\n",
    "        print(f\"Summarized document saved successfully as {OUTPUT_PDF_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Process failed: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
